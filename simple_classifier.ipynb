{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 |Anaconda 4.1.1 (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymysql\n",
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import warnings\n",
    "import shutil\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.sparse import coo_matrix, vstack,csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Tokenization\n",
    "#2. Lematization\n",
    "#3. Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.tokenize\n",
    "import nltk.data\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(columns=[\"dk_ref\",\"fname\",\"decision_fk\",\"lawyer_name\",\"text_data\"])\n",
    "db = pymysql.connect(host=\"localhost\", port=3320, user=\"vicky\", passwd=\"vicky9790851962\", db=\"fulltextdarts\", charset=\"utf8\")\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query=\"SELECT dk.reference, filename, decision_fk,euipodl.lawyer_name,textdata FROM fulltextdarts.plaintext pt JOIN darts.decision d ON pt.decision_fk=d.id JOIN darts.court c ON court_fk=c.id JOIN darts.docket dk ON d.docket_fk=dk.id JOIN judges.euipo_docket_lawyer euipodl ON euipodl.opp_number=dk.reference WHERE c.id=1121 AND pt.language='en' AND dk.reference REGEXP '[0-9]{9}';\"\n",
    "# cursor.execute(query)\n",
    "\n",
    "# i=0\n",
    "# for row in cursor:\n",
    "#     data.loc[i]=row;i+=1\n",
    "\n",
    "#pickle.dump(data,open(\"euipo_en_data.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pickle.load(open(\"euipo_en_data.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count_vec=CountVectorizer(tokenizer=word_tokenize)\n",
    "# count_vec.fit_transform(data.loc[:,\"text_data\"])\n",
    "\n",
    "# pickle.dump(count_vec,open(\"euipo_en_count_vec.p\",\"wb\"))\n",
    "# pickle.dump(count_vec.vocabulary_,open(\"euipo_en_vocab.p\",\"wb\"))\n",
    "count_vec=CountVectorizer(tokenizer=word_tokenize,vocabulary=pickle.load(open(\"euipo_en_vocab.p\",\"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_len=3\n",
    "tokens=[word for word in word_tokenize(data.loc[0,\"text_data\"])]\n",
    "tokens_SS=[\"START\"]*context_len+tokens+[\"STOP\"]*context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grams=[gram for gram in nltk.ngrams(tokens_SS,context_len*2+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for token,gram in zip(tokens[context_len:],grams):\n",
    "#     if token!=gram[context_len]:\n",
    "#         print token,gram[context_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_l3</th>\n",
       "      <th>context_l2</th>\n",
       "      <th>context_l1</th>\n",
       "      <th>word</th>\n",
       "      <th>context_r1</th>\n",
       "      <th>context_r2</th>\n",
       "      <th>context_r3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START</td>\n",
       "      <td>START</td>\n",
       "      <td>START</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>FOR</td>\n",
       "      <td>HARMONIZATION</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>START</td>\n",
       "      <td>START</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>FOR</td>\n",
       "      <td>HARMONIZATION</td>\n",
       "      <td>IN</td>\n",
       "      <td>THE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>START</td>\n",
       "      <td>OFFICE</td>\n",
       "      <td>FOR</td>\n",
       "      <td>HARMONIZATION</td>\n",
       "      <td>IN</td>\n",
       "      <td>THE</td>\n",
       "      <td>INTERNAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OFFICE</td>\n",
       "      <td>FOR</td>\n",
       "      <td>HARMONIZATION</td>\n",
       "      <td>IN</td>\n",
       "      <td>THE</td>\n",
       "      <td>INTERNAL</td>\n",
       "      <td>MARKET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOR</td>\n",
       "      <td>HARMONIZATION</td>\n",
       "      <td>IN</td>\n",
       "      <td>THE</td>\n",
       "      <td>INTERNAL</td>\n",
       "      <td>MARKET</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  context_l3     context_l2     context_l1           word     context_r1  \\\n",
       "0      START          START          START         OFFICE            FOR   \n",
       "1      START          START         OFFICE            FOR  HARMONIZATION   \n",
       "2      START         OFFICE            FOR  HARMONIZATION             IN   \n",
       "3     OFFICE            FOR  HARMONIZATION             IN            THE   \n",
       "4        FOR  HARMONIZATION             IN            THE       INTERNAL   \n",
       "\n",
       "      context_r2 context_r3  \n",
       "0  HARMONIZATION         IN  \n",
       "1             IN        THE  \n",
       "2            THE   INTERNAL  \n",
       "3       INTERNAL     MARKET  \n",
       "4         MARKET          (  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df=pd.DataFrame(index=range(len(tokens)),columns=[\"context_l3\",\"context_l2\",\"context_l1\",\"word\",\"context_r1\",\"context_r2\",\"context_r3\"])\n",
    "\n",
    "features_df.loc[:]=grams[:]\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grams_list=[unicode(\" \".join(gram)) for gram in grams]\n",
    "context_features=count_vec.transform(grams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features=[]\n",
    "for word in tokens_SS:\n",
    "    word_features.append([int(word.isalnum()),int(word.isalpha()),int(word.islower()),int(word.isupper())])\n",
    "\n",
    "word_features_grams=nltk.ngrams(word_features,context_len*2+1)\n",
    "word_features_array=np.ndarray.flatten(np.asarray(list(word_features_grams))[:]).reshape((len(tokens),4*(context_len*2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_df_2=pd.DataFrame(index=range(len(tokens)),columns=[\"context_l3_alnum\",\"context_l3_alpha\",\"context_l3_lower\",\"context_l3_upper\",\"context_l2_alnum\",\"context_l2_alpha\",\"context_l2_lower\",\"context_l2_upper\",\"context_l1_alnum\",\"context_l1_alpha\",\"context_l1_lower\",\"context_l1_upper\",\"word_alnum\",\"word_alpha\",\"word_lower\",\"word_upper\",\"context_r1_alnum\",\"context_r1_alpha\",\"context_r1_lower\",\"context_r1_upper\",\"context_r2_alnum\",\"context_r2_alpha\",\"context_r2_lower\",\"context_r2_upper\",\"context_r3_alnum\",\"context_r3_alpha\",\"context_r3_lower\",\"context_r3_upper\"])\n",
    "features_df_2.loc[:]=word_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 217174)\n",
      "(3428, 7)\n",
      "(3428, 28)\n"
     ]
    }
   ],
   "source": [
    "#print word_features_array.shape\n",
    "print context_features.shape\n",
    "print features_df.shape\n",
    "print features_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "judges_df=pd.read_excel(open(\"training_set.xlsx\",\"rb\"),parse_cols=\"E,F,G\")\n",
    "\n",
    "temp_df=pd.DataFrame(columns=[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"])\n",
    "\n",
    "frames=[data,temp_df]\n",
    "full_data=pd.concat(frames)\n",
    "\n",
    "full_data.loc[0:len(judges_df),[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"]]=judges_df.loc[0:len(judges_df),[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label=np.zeros((len(tokens)))\n",
    "for token in word_tokenize(full_data.judge_name_1[0]):\n",
    "    label[tokens.index(token)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df=pd.DataFrame(columns=[\"Context_Features\",\"Word_Features\",\"Labels\"])\n",
    "frames=[full_data,temp_df]\n",
    "labelled_data=pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
