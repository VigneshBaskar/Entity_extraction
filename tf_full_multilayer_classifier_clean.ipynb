{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pymysql\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk import ngrams\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk.tokenize\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pymysql\n",
    "# db = pymysql.connect(host=\"localhost\", port=3306, user=\"vicky\", passwd=\"vicky9790851962\", db=\"fulltextdarts\", charset=\"utf8\")\n",
    "# cursor = db.cursor()\n",
    "# query=\"SELECT DISTINCT filename, textdata FROM fulltextdarts.plaintext pt JOIN darts.decision d ON pt.decision_fk=d.id JOIN darts.court c ON court_fk=c.id JOIN darts.docket dk ON d.docket_fk=dk.id JOIN judges.euipo_docket_lawyer euipodl ON euipodl.opp_number=dk.reference WHERE c.id=1121 AND pt.language='en' AND dk.reference REGEXP '[0-9]{9}' ORDER BY RAND(123);\"\n",
    "# cursor.execute(query)\n",
    "\n",
    "\n",
    "# data=pd.DataFrame(columns=[\"filename\",\"text_data\"])\n",
    "# i=0\n",
    "# for row in cursor:\n",
    "#     data.loc[i]=row;i+=1\n",
    "\n",
    "# judges_df=pd.read_excel(open(\"tf_training_set.xlsx\",\"rb\"),parse_cols=\"B,C,D\")\n",
    "\n",
    "# temp_df=pd.DataFrame(columns=[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"])\n",
    "\n",
    "# frames=[data,temp_df]\n",
    "# full_data=pd.concat(frames)\n",
    "\n",
    "# full_data.loc[0:len(judges_df),[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"]]=judges_df.loc[0:len(judges_df),[\"judge_name_1\",\"judge_name_2\",\"judge_name_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences=[full_data['text_data'][index].lower() for index in range(len(full_data['text_data']))]\n",
    "# model=Word2Vec([word_tokenize(s) for s in sentences], size=150, min_count=1, window=5,  workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle.dump(full_data,open(\"full_data.p\",\"wb\"))\n",
    "# pickle.dump(model,open(\"model.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data=pickle.load(open(\"full_data.p\",\"rb\"))\n",
    "model=pickle.load(open(\"model.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for index in range(70):\n",
    "#     print index\n",
    "#     count=0\n",
    "#     for judge_name in [full_data.judge_name_1[index],full_data.judge_name_2[index],full_data.judge_name_3[index]]:\n",
    "#         tokenized_judge_name= word_tokenize(judge_name)\n",
    "\n",
    "#         print \"B-JN is \"+tokenized_judge_name[0]+\" \"+str(count+1)\n",
    "\n",
    "#         try:\n",
    "#             for word in tokenized_judge_name[1:-1]:\n",
    "#                 print \"I-JN is \"+word+\" \"+str(count+1)\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         print \"E-JN is \"+tokenized_judge_name[-1]+\" \"+str(count+1)\n",
    "#         print \"\\n\"\n",
    "#         count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_w2v_gram_features_extraction(text,context_len,word2vec_length=150):\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens_lower=[word.lower() for word in tokens]\n",
    "\n",
    "    tokens_SS=[\"DOCSTART\"]*context_len+word_tokenize(text)+[\"DOCSTOP\"]*context_len\n",
    "    tokens_SS_lower=[word.lower() for word in tokens_SS]\n",
    "    tokens_SS_shape_features=[[int(unicode(word).isnumeric()),int(unicode(word).isalpha()),int(unicode(word).islower()),int(unicode(word).isupper())] for word in tokens_SS]\n",
    "\n",
    "    word2vec_features=[]\n",
    "    for word in tokens_lower:\n",
    "        try:\n",
    "            word2vec_features.append(list(model[word]))\n",
    "        except:\n",
    "            word2vec_features.append(list(np.random.rand(word2vec_length,1)))\n",
    "    \n",
    "    word2vec_SS_features=[list(np.zeros_like((word2vec_features[0])))]*context_len+word2vec_features+[list(np.zeros_like((word2vec_features[0])))]*context_len\n",
    "\n",
    "\n",
    "    tokens_SS_grams = list(ngrams(tokens_SS, context_len*2+1))\n",
    "    tokens_SS_shape_features_grams = np.asarray(list(ngrams(tokens_SS_shape_features, context_len*2+1)))\n",
    "    word2vec_SS_features_grams=np.asarray(list(ngrams(word2vec_SS_features, context_len*2+1)))\n",
    "\n",
    "\n",
    "    check1=tokens_SS_shape_features_grams.reshape(len(tokens_SS_grams),-1)\n",
    "    check2=word2vec_SS_features_grams.reshape(len(tokens_SS_grams),-1)\n",
    "    shape_w2v_gram_features=np.concatenate((check1,check2),axis=1)\n",
    "    return tokens,shape_w2v_gram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_training_set_labels_extraction(text,judge_names):\n",
    "    tokens=word_tokenize(text)\n",
    "    y_label=np.zeros(len(tokens))\n",
    "\n",
    "    if judge_names[1]==\" \":\n",
    "            tokenized_judge_name= word_tokenize(judge_names[0])\n",
    "            print tokenized_judge_name\n",
    "            y_label[tokens.index(tokenized_judge_name[0])]=1\n",
    "\n",
    "            try:\n",
    "                for word in tokenized_judge_name[1:-1]:\n",
    "                    y_label[tokens.index(word)]=2\n",
    "            except:\n",
    "                pass\n",
    "            y_label[tokens.index(tokenized_judge_name[-1])]=3\n",
    "\n",
    "    else:\n",
    "        count=0\n",
    "        for judge_name in judge_names:\n",
    "            tokenized_judge_name= word_tokenize(judge_name)\n",
    "\n",
    "            y_label[tokens.index(tokenized_judge_name[0])]=count+1\n",
    "\n",
    "            try:\n",
    "                for word in tokenized_judge_name[1:-1]:\n",
    "                    y_label[tokens.index(word)]=count+1\n",
    "            except:\n",
    "                pass\n",
    "            y_label[tokens.index(tokenized_judge_name[-1])]=count+1\n",
    "            count+=1\n",
    "    return tokens,y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_test_set_labels_extraction(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    y_label=np.zeros(len(tokens))\n",
    "    return tokens,y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Mariya', u'KOLEVA']\n"
     ]
    }
   ],
   "source": [
    "data_tokens=[]\n",
    "data_features=[]\n",
    "data_labels=[]\n",
    "data_filenames=[]\n",
    "\n",
    "full_data=full_data.fillna(\" \")\n",
    "\n",
    "context_len=3\n",
    "for index in range(200):\n",
    "\n",
    "    text=full_data.loc[index,\"text_data\"]\n",
    "    judge_names=[full_data.judge_name_1[index],full_data.judge_name_2[index],full_data.judge_name_3[index]]\n",
    "\n",
    "    tokens,shape_w2v_gram_features=shape_w2v_gram_features_extraction(text,context_len)\n",
    "    if index<79:\n",
    "        _,y_label=tf_training_set_labels_extraction(text,judge_names)\n",
    "    else:\n",
    "        _,y_label=tf_test_set_labels_extraction(text)\n",
    "\n",
    "#     print np.asarray(tokens)[y_label!=0]\n",
    "#     print np.asarray(y_label)[y_label!=0]\n",
    "    data_filenames.append(full_data.loc[index,\"filename\"])\n",
    "    data_tokens.append(tokens)\n",
    "    data_features.append(shape_w2v_gram_features)\n",
    "    data_labels.append(y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_features=np.asarray(data_features)\n",
    "data_labels=np.asarray(data_labels)\n",
    "data_tokens=np.asarray(data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = 4\n",
    "def reformat(labels):\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset=np.vstack(data_features[0:40])\n",
    "train_dataset=train_dataset.astype(np.float32)\n",
    "train_labels=reformat(np.hstack(data_labels[0:40]))\n",
    "train_tokens=np.hstack(data_tokens[0:40])\n",
    "\n",
    "valid_dataset=np.vstack(data_features[40:55])\n",
    "valid_dataset=valid_dataset.astype(np.float32)\n",
    "valid_labels=reformat(np.hstack(data_labels[40:55]))\n",
    "valid_tokens=np.hstack(data_tokens[40:55])\n",
    "\n",
    "test_dataset=np.vstack(data_features[55:79])\n",
    "test_dataset=test_dataset.astype(np.float32)\n",
    "test_labels=reformat(np.hstack(data_labels[55:79]))\n",
    "test_tokens=np.hstack(data_tokens[55:79])\n",
    "\n",
    "#print np.sum(test_labels,0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "feature_size=train_dataset.shape[1]\n",
    "num_labels=train_labels.shape[1]\n",
    "\n",
    "graph1=tf.Graph()\n",
    "with graph1.as_default():\n",
    "    \n",
    "    tf_train_dataset=tf.placeholder(tf.float32,shape=(batch_size,feature_size))\n",
    "    tf_train_labels=tf.placeholder(tf.float32,shape=(batch_size,num_labels))\n",
    "    \n",
    "    tf_valid_dataset=tf.constant(valid_dataset)\n",
    "    \n",
    "    tf_test_dataset=tf.placeholder(tf.float32,shape=(None,feature_size))\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([feature_size , int(feature_size*0.7) ]))\n",
    "    biases1 = tf.Variable(tf.zeros([int(feature_size*0.7) ]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([int(feature_size*0.7) , int(feature_size*0.5) ]))\n",
    "    biases2 = tf.Variable(tf.zeros([int(feature_size*0.5) ]))     \n",
    "    weights3 = tf.Variable(tf.truncated_normal([int(feature_size*0.5) , num_labels]))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels])) \n",
    "\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "    logits1 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(tf_train_dataset, weights1) + biases1),keep_prob=0.9)\n",
    "    logits2 =  tf.nn.dropout(tf.nn.relu(tf.matmul(logits1,weights2)+biases2),keep_prob=0.9)\n",
    "    logits =  tf.matmul(logits2,weights3)+biases3  \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_valid_dataset, weights1) + biases1),weights2)+biases2),weights3)+biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, weights1) + biases1),weights2)+biases2),weights3)+biases3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all the variables\n",
      "Minibatch loss at step 0: 445.999939\n",
      "Minibatch accuracy: 3.5%\n",
      "Validation accuracy: 99.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.9%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph1) as session1:\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized all the variables\")\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        offset=(step*batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data=train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels=train_labels[offset:(offset+batch_size),:]\n",
    "        \n",
    "        feed_dict={tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        _,l,predictions=session1.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "    weights_saved=[weights1.eval(),weights2.eval(),weights3.eval(),biases1.eval(),biases2.eval(),biases3.eval()]\n",
    "[nn1_weights1,nn1_weights2,nn1_weights3,nn1_biases1,nn1_biases2,nn1_biases3]=weights_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph2=tf.Graph()\n",
    "with graph2.as_default():    \n",
    "    tf_test_dataset=tf.placeholder(tf.float32,shape=(None,feature_size))\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn1_weights1) + nn1_biases1),nn1_weights2)+nn1_biases2),nn1_weights3)+nn1_biases3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph2) as session2:\n",
    "    tf.initialize_all_variables().run()\n",
    "    feed_dict={tf_test_dataset:train_dataset}\n",
    "    nn1_train_predictions=session2.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    feed_dict={tf_test_dataset:valid_dataset}\n",
    "    nn1_valid_predictions=session2.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    feed_dict={tf_test_dataset:test_dataset}\n",
    "    nn1_test_predictions=session2.run([test_prediction],feed_dict=feed_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "145\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "print np.sum(np.argmax(nn1_test_predictions[nn1_test_predictions[:,0]!=1],1)==np.argmax(test_labels[nn1_test_predictions[:,0]!=1],1))\n",
    "print len(nn1_test_predictions[nn1_test_predictions[:,0]!=1])\n",
    "print len(test_labels[test_labels[:,0]!=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ardiel 1 1\n",
      "CABRERA 1 1\n",
      "RUIZ 2 1\n",
      "Lina 2 2\n",
      "LAPINSKAITE 2 2\n",
      "Justyna 3 3\n",
      "GBYL 3 3\n",
      "Isabel 1 1\n",
      "De 1 1\n",
      "ALFONSETI 2 1\n",
      "HARTMANN 3 1\n",
      "Julio 2 2\n",
      "LAPORTA 3 2\n",
      "INSA 3 2\n",
      "Mira 2 3\n",
      "RAJH 2 3\n",
      "Andras 1 2\n",
      "SZASZ 1 2\n",
      "Antoneta 3 3\n",
      "CVETIC 3 3\n",
      "Daniel 1 1\n",
      "Meredydd 1 1\n",
      "Anthony 2 1\n",
      "Ralph 2 2\n",
      "Pethke 2 2\n",
      "Sigrid 3 3\n",
      "Dickmanns 3 3\n",
      "Kaidi 1 1\n",
      "TOOMUS 1 1\n",
      "Lina 2 2\n",
      "LAPINSKAITE 2 2\n",
      "Margarida 3 3\n",
      "CHAMBEL 3 3\n",
      "GONÇALVES 3 3\n",
      "Justas 1 1\n",
      "IVANAUSKAS 1 1\n",
      "Birgit 2 2\n",
      "Holst 2 2\n",
      "FILTENBORG 2 2\n",
      "Silvie 0 3\n",
      "4 3 0\n",
      "Richard 1 1\n",
      "THEWLIS 1 1\n",
      "William 2 2\n",
      "TROTT 3 2\n",
      "Helena 3 3\n",
      "NOKKANEN 3 3\n",
      "Erkki 1 1\n",
      "MÜNTER 1 1\n",
      "Ricardo 2 2\n",
      "NUNES 2 2\n",
      "FERREIRA 2 2\n",
      "Natascha 3 3\n",
      "SEMJEVSKI 3 3\n",
      "Alain 1 1\n",
      "RASSAT 1 1\n",
      "Vanessa 2 2\n",
      "Patricia 3 3\n",
      "DE 3 3\n",
      "CORRES 3 3\n",
      "Agnes 1 1\n",
      "SZANYI 1 1\n",
      "Richard 2 2\n",
      "BIANCHI 2 2\n",
      "Agueda 2 3\n",
      "Las 0 2\n",
      "Kapnopoulou 3 3\n",
      "Mauro 1 1\n",
      "BUFFOLO 1 1\n",
      "Ralph 2 2\n",
      "PETHKE 2 2\n",
      "Anna 3 3\n",
      "POLITI 3 3\n",
      "Óscar 1 1\n",
      "Mondéjar 1 1\n",
      "Deirdre 2 2\n",
      "Quinn 3 2\n",
      "Zoltán 3 3\n",
      "Szarka 3 3\n",
      "Birgit 1 1\n",
      "FILTENBORG 1 1\n",
      "Raluca 2 2\n",
      "ARDELEANU 2 2\n",
      "Paul 3 3\n",
      "BULLOCK 3 3\n",
      "Luca 1 1\n",
      "RAMPINI 1 1\n",
      "Mira 2 2\n",
      "RAJH 2 2\n",
      "Ana 2 3\n",
      "María 3 0\n",
      "PAREJA 3 3\n",
      "TORRES 3 3\n",
      "Julio 1 1\n",
      "LAPORTA 1 1\n",
      "INSA 2 1\n",
      "Nicole 2 2\n",
      "CLARKE 2 2\n",
      "Mariya 1 1\n",
      "KOLEVA 3 3\n",
      "da 2 0\n",
      "Liliya 1 1\n",
      "YORDANOVA 1 1\n",
      "Astrid 2 2\n",
      "GRAUL 3 2\n",
      "Richard 1 1\n",
      "THEWLIS 1 1\n",
      "Robert 2 2\n",
      "KLIJN 2 2\n",
      "BRINKEMA 3 2\n",
      "Daniel 3 3\n",
      "GAJA 3 3\n",
      "Judit 1 1\n",
      "NÉMETH 1 1\n",
      "Alexander 2 2\n",
      "SCHIFKO 2 2\n",
      "Juan 2 3\n",
      "Antonio 2 3\n",
      "MORALES 3 3\n",
      "PAREDES 3 3\n",
      "Marc 1 1\n",
      "BURKI 1 1\n",
      "Frédérique 2 2\n",
      "SULPICE 2 2\n",
      "Vanessa 3 3\n",
      "PAGE 3 3\n",
      "Birgit 1 1\n",
      "FILTENBORG 1 1\n",
      "Chantal 2 2\n",
      "VAN 2 2\n",
      "RIEL 2 2\n",
      "Louise 3 3\n",
      "THORNING 3 3\n",
      "Frédérique 1 1\n",
      "Sulpice 1 1\n",
      "Anne-Lee 2 2\n",
      "Kristensen 2 2\n",
      "Holger 3 3\n",
      "Kunz 3 3\n",
      "Ewa 1 1\n",
      "MISZCZAK 1 1\n",
      "Andrea 2 2\n",
      "NEWMAN 2 2\n",
      "Alexandre 3 3\n",
      "BERSET 3 3\n"
     ]
    }
   ],
   "source": [
    "#print test_dataset.shape, nn1_test_predictions.shape, test_labels.shape, len(test_tokens)\n",
    "for x,y,z in zip(test_tokens[nn1_test_predictions[:,0]!=1],np.argmax(nn1_test_predictions[nn1_test_predictions[:,0]!=1],1),np.argmax(test_labels[nn1_test_predictions[:,0]!=1],1)):\n",
    "    print x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "[u'Julio' u'LAPORTA' u'INSA' u'Nicole' u'CLARKE']\n",
      "[1 1 2 2 2]\n",
      " \n",
      "71\n",
      "[u'Mariya' u'KOLEVA']\n",
      "[1 3]\n",
      " \n",
      "72\n",
      "[u'da' u'Liliya' u'YORDANOVA' u'Astrid' u'GRAUL']\n",
      "[2 1 1 2 3]\n",
      " \n",
      "73\n",
      "[u'Richard' u'THEWLIS' u'Robert' u'KLIJN' u'BRINKEMA' u'Daniel' u'GAJA']\n",
      "[1 1 2 2 3 3 3]\n",
      " \n",
      "74\n",
      "[u'Judit' u'N\\xc9METH' u'Alexander' u'SCHIFKO' u'Juan' u'Antonio'\n",
      " u'MORALES' u'PAREDES']\n",
      "[1 1 2 2 2 2 3 3]\n",
      " \n",
      "75\n",
      "[u'Marc' u'BURKI' u'Fr\\xe9d\\xe9rique' u'SULPICE' u'Vanessa' u'PAGE']\n",
      "[1 1 2 2 3 3]\n",
      " \n",
      "76\n",
      "[u'Birgit' u'FILTENBORG' u'Chantal' u'VAN' u'RIEL' u'Louise' u'THORNING']\n",
      "[1 1 2 2 2 3 3]\n",
      " \n",
      "77\n",
      "[u'Fr\\xe9d\\xe9rique' u'Sulpice' u'Anne-Lee' u'Kristensen' u'Holger' u'Kunz']\n",
      "[1 1 2 2 3 3]\n",
      " \n",
      "78\n",
      "[u'Ewa' u'MISZCZAK' u'Andrea' u'NEWMAN' u'Alexandre' u'BERSET']\n",
      "[1 1 2 2 3 3]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in range(70,79):\n",
    "    print index\n",
    "    single_test_dataset=data_features[index].astype(\"float32\")\n",
    "    single_test_labels=data_labels[index]\n",
    "    single_test_tokens=np.asarray(data_tokens[index])\n",
    "\n",
    "    with tf.Session(graph=graph2) as session2:\n",
    "        tf.initialize_all_variables().run()  \n",
    "        feed_dict={tf_test_dataset:single_test_dataset}\n",
    "        nn1_single_test_predictions=session2.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "\n",
    "    print single_test_tokens[nn1_single_test_predictions[:,0]!=1]\n",
    "    #print single_test_tokens[np.asarray(single_test_labels)!=0]\n",
    "    print np.argmax(nn1_single_test_predictions[nn1_single_test_predictions[:,0]!=1],1)\n",
    "    #print single_test_labels[nn1_single_test_predictions[:,0]!=1]\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_len_nn_2=6\n",
    "def nn2_input_feature_extraction(nn1_predictions,context_len_nn_2,num_labels=4):\n",
    "    temp=np.vstack((np.zeros((context_len_nn_2,num_labels)),nn1_predictions))\n",
    "    temp2=np.vstack((temp,np.zeros((context_len_nn_2,num_labels))))\n",
    "    temp2_ngrams=np.asarray(list(ngrams(temp2,context_len_nn_2*2+1)))\n",
    "    temp2_ngrams=temp2_ngrams.astype(\"float32\")\n",
    "    return temp2_ngrams.reshape(len(temp2_ngrams),-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.hstack((nn1_train_predictions_grams,train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn1_train_predictions_grams=nn2_input_feature_extraction(nn1_train_predictions,context_len_nn_2,num_labels)\n",
    "nn1_train_predictions_grams=np.hstack((nn1_train_predictions_grams,train_dataset))\n",
    "nn1_valid_predictions_grams=nn2_input_feature_extraction(nn1_valid_predictions,context_len_nn_2,num_labels)\n",
    "nn1_valid_predictions_grams=np.hstack((nn1_valid_predictions_grams,valid_dataset))\n",
    "nn1_test_predictions_grams=nn2_input_feature_extraction(nn1_test_predictions,context_len_nn_2,num_labels)\n",
    "nn1_test_predictions_grams=np.hstack((nn1_test_predictions_grams,test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "feature_size=nn1_train_predictions_grams.shape[1]\n",
    "num_labels=train_labels.shape[1]\n",
    "\n",
    "graph3=tf.Graph()\n",
    "with graph3.as_default():\n",
    "    \n",
    "    nn2_tf_train_dataset=tf.placeholder(tf.float32,shape=(batch_size,feature_size))\n",
    "    nn2_tf_train_labels=tf.placeholder(tf.float32,shape=(batch_size,num_labels))\n",
    "    nn2_tf_valid_dataset=tf.constant(nn1_valid_predictions_grams)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Variables.\n",
    "    nn2_weights1 = tf.Variable(tf.truncated_normal([feature_size , int(feature_size*0.7) ]))\n",
    "    nn2_biases1 = tf.Variable(tf.zeros([int(feature_size*0.7) ]))\n",
    "    \n",
    "    nn2_weights2 = tf.Variable(tf.truncated_normal([int(feature_size*0.7) , int(feature_size*0.5) ]))\n",
    "    nn2_biases2 = tf.Variable(tf.zeros([int(feature_size*0.5) ]))\n",
    "   \n",
    "    nn2_weights3 = tf.Variable(tf.truncated_normal([int(feature_size*0.5) , num_labels]))\n",
    "    nn2_biases3 = tf.Variable(tf.zeros([num_labels])) \n",
    "\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "    nn2_logits1 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(nn2_tf_train_dataset, nn2_weights1) + nn2_biases1),keep_prob=0.9)\n",
    "    nn2_logits2 = tf.nn.dropout(tf.nn.relu(tf.matmul(nn2_logits1,nn2_weights2)+nn2_biases2),keep_prob=0.9)\n",
    "    nn2_logits =  tf.matmul(nn2_logits2,nn2_weights3)+nn2_biases3\n",
    "    nn2_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(nn2_logits, nn2_tf_train_labels))\n",
    "    nn2_train_prediction = tf.nn.softmax(nn2_logits)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.25).minimize(nn2_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    #valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(nn2_tf_valid_dataset, nn2_weights1) + nn2_biases1),nn2_weights2)+nn2_biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(nn2_tf_valid_dataset, nn2_weights1) + nn2_biases1),nn2_weights2)+nn2_biases2),nn2_weights3)+nn2_biases3)    \n",
    "    #test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(nn2_tf_test_dataset, nn2_weights1) + nn2_biases1),nn2_weights2)+nn2_biases2),nn2_weights3)+nn2_biases3)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all the variables\n",
      "Minibatch loss at step 0: 13.444063\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 99.8%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph3) as session3:\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized all the variables\")\n",
    "    \n",
    "    #for num in range(3):\n",
    "    for step in range(num_steps):\n",
    "        offset=(step*batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data=nn1_train_predictions_grams[offset:(offset+batch_size),:]\n",
    "        batch_labels=train_labels[offset:(offset+batch_size),:]\n",
    "\n",
    "\n",
    "        feed_dict={nn2_tf_train_dataset:batch_data,nn2_tf_train_labels:batch_labels}\n",
    "        _,l,predictions=session3.run([optimizer, nn2_loss, nn2_train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "    weights_saved=[nn2_weights1.eval(),nn2_weights2.eval(),nn2_weights3.eval(),nn2_biases1.eval(),nn2_biases2.eval(),nn2_biases3.eval()]\n",
    "    #weights_saved=[nn2_weights1.eval(),nn2_weights2.eval(),nn2_biases1.eval(),nn2_biases2.eval()]\n",
    "#[nn2_weights1_saved,nn2_weights2_saved,nn2_biases1_saved,nn2_biases2_saved]=weights_saved\n",
    "[nn2_weights1_saved,nn2_weights2_saved,nn2_weights3_saved,nn2_biases1_saved,nn2_biases2_saved,nn2_biases3_saved]=weights_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph4=tf.Graph()\n",
    "with graph4.as_default():    \n",
    "    tf_test_dataset=tf.placeholder(tf.float32,shape=(None,feature_size))\n",
    "    #test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved),nn2_weights3_saved)+nn2_biases3_saved)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph4) as session4:\n",
    "    tf.initialize_all_variables().run()\n",
    "    feed_dict={tf_test_dataset:nn1_train_predictions_grams}\n",
    "    nn2_train_predictions=session4.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    feed_dict={tf_test_dataset:nn1_valid_predictions_grams}\n",
    "    nn2_valid_predictions=session4.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    feed_dict={tf_test_dataset:nn1_test_predictions_grams}\n",
    "    nn2_test_predictions=session4.run([test_prediction],feed_dict=feed_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "143\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "print np.sum(np.argmax(nn2_test_predictions[nn2_test_predictions[:,0]<0.5],1)==np.argmax(test_labels[nn2_test_predictions[:,0]<0.5],1))\n",
    "print len(nn2_test_predictions[nn1_test_predictions[:,0]<0.5])\n",
    "print len(test_labels[test_labels[:,0]!=1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "80\n",
      "Loreto URRACA\n",
      "LUQUE Ana PAREJA\n",
      "Paul BULLOCK\n",
      " \n",
      "Loreto URRACA\n",
      "LUQUE Ana PAREJA\n",
      "Paul BULLOCK\n",
      " \n",
      "81\n",
      "Marcela Langerová\n",
      "Ricardo Nunes Ferreira\n",
      "Janja Felc\n",
      " \n",
      "Marcela Langerová\n",
      "Ricardo Nunes\n",
      "Ferreira Janja Felc\n",
      " \n",
      "82\n",
      "David LEFFLER\n",
      "Andrea NEWMAN\n",
      "Raoul ERARD\n",
      " \n",
      "David LEFFLER\n",
      "Andrea\n",
      "Raoul ERARD\n",
      " \n",
      "83\n",
      "David LEFFLER\n",
      "Hugh O’NEILL\n",
      "Lucie TRPÍKOVÁ\n",
      " \n",
      "David LEFFLER\n",
      "Hugh O’NEILL\n",
      "Lucie TRPÍKOVÁ\n",
      " \n",
      "84\n",
      "Ardiel CABRERA\n",
      "RUIZ Kaidi TOOMUS\n",
      "Julio LAPORTA\n",
      " \n",
      "Ardiel CABRERA\n",
      "RUIZ Kaidi\n",
      "TOOMUS Julio LAPORTA\n",
      " \n",
      "85\n",
      "Deirdre QUINN\n",
      "Sandrine\n",
      "Paul BULLOCK\n",
      " \n",
      "Deirdre QUINN\n",
      "Sandrine BUORO\n",
      "Paul BULLOCK\n",
      " \n",
      "86\n",
      "dissect Swetlana BRAUN\n",
      "Ewelina\n",
      "\n",
      " \n",
      "Swetlana BRAUN\n",
      "Ewelina ŚLIWIŃSKA\n",
      "Silvie\n",
      " \n",
      "87\n",
      "Daniel GÁJA\n",
      "Mira\n",
      "RAJH Radka STUPKOVÁ\n",
      " \n",
      "Daniel GÁJA\n",
      "Mira\n",
      "Radka STUPKOVÁ\n",
      " \n",
      "88\n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Margarida CHAMBEL\n",
      " \n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Margarida CHAMBEL\n",
      " \n",
      "89\n",
      "lsabel DE\n",
      "ALFONSETI URRACA LUQUE Alvaro\n",
      "Loreto SESMA MERINO\n",
      " \n",
      "lsabel DE ALFONSETI\n",
      "URRACA LUQUE\n",
      "Alvaro SESMA MERINO\n",
      " \n",
      "90\n",
      "Patricia LOPEZ\n",
      "FERNANDEZ CORRES Holger\n",
      "Vanessa PAGE\n",
      " \n",
      "Patricia LOPEZ\n",
      "\n",
      "Holger KUNZ Vanessa PAGE\n",
      " \n",
      "91\n",
      "Manuela MIEHLE\n",
      "Richard THEWLIS\n",
      "Daniel GÁJA\n",
      " \n",
      "Manuela MIEHLE\n",
      "Richard THEWLIS\n",
      "Daniel GÁJA\n",
      " \n",
      "92\n",
      "Jorge NOVAIS\n",
      "GONÇALVES Dorothee\n",
      "SCHLIEPHAKE Elena FRAILE\n",
      " \n",
      "Jorge NOVAIS\n",
      "GONÇALVES Dorothee\n",
      "SCHLIEPHAKE Elena FRAILE\n",
      " \n",
      "93\n",
      "Dorothée SCHLIEPHAKE\n",
      "Luca RAMPINI\n",
      "Jorge NOVAIS GONÇALVES\n",
      " \n",
      "Dorothée SCHLIEPHAKE\n",
      "Luca RAMPINI\n",
      "Jorge NOVAIS GONÇALVES\n",
      " \n",
      "94\n",
      "Daniel CAMPAGNE\n",
      "Kelly-Marie BENNETT\n",
      "Daniel GAJA\n",
      " \n",
      "Daniel CAMPAGNE\n",
      "Kelly-Marie BENNETT\n",
      "Daniel GAJA\n",
      " \n",
      "95\n",
      "Liliya Yordanova\n",
      "Alexander Wickenhöfer Thomas\n",
      "D. Nicholson\n",
      " \n",
      "Liliya Yordanova\n",
      "Alexander Wickenhöfer\n",
      "D. Nicholson\n",
      " \n",
      "96\n",
      "Alessandro SEMPIO\n",
      "Mauro BUFFalO\n",
      "luca RAMPINI\n",
      " \n",
      "Alessandro SEMPIO\n",
      "Mauro\n",
      "BUFFalO luca RAMPINI\n",
      " \n",
      "97\n",
      "Susanna LAITINEN\n",
      "Mira\n",
      "Silvié GÖTZOVA\n",
      " \n",
      "Susanna LAITINEN\n",
      "Mira\n",
      "Silvié GÖTZOVA\n",
      " \n",
      "98\n",
      "Miroslav STANČÍK\n",
      "Ric WASLEY\n",
      "Silvie GOTZOVA\n",
      " \n",
      "Miroslav STANČÍK\n",
      "Ric WASLEY\n",
      "Silvie GOTZOVA\n",
      " \n",
      "99\n",
      "Andrea NEWMAN\n",
      "Ewa MISZCZAK\n",
      "Raoul ERARD\n",
      " \n",
      "Andrea NEWMAN\n",
      "Ewa\n",
      "MISZCZAK Raoul ERARD\n"
     ]
    }
   ],
   "source": [
    "for index in range(80,100):\n",
    "    print(\" \")\n",
    "    print index\n",
    "    single_test_dataset=data_features[index].astype(\"float32\")\n",
    "    single_test_labels=data_labels[index]\n",
    "    single_test_tokens=np.asarray(data_tokens[index])\n",
    "    \n",
    "    graph5=tf.Graph()\n",
    "    with graph5.as_default():    \n",
    "        tf_test_dataset=tf.placeholder(tf.float32,shape=(None,None))\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn1_weights1) + nn1_biases1),nn1_weights2)+nn1_biases2),nn1_weights3)+nn1_biases3)    \n",
    "        \n",
    "    with tf.Session(graph=graph5) as session5:\n",
    "        tf.initialize_all_variables().run()  \n",
    "        feed_dict={tf_test_dataset:single_test_dataset}\n",
    "        nn1_single_test_predictions=session5.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "        \n",
    "    nn1_single_test_predictions_grams=nn2_input_feature_extraction(nn1_single_test_predictions,context_len_nn_2,num_labels)\n",
    "    nn1_single_test_predictions_grams=np.hstack((nn1_single_test_predictions_grams,single_test_dataset))\n",
    "    \n",
    "    graph6=tf.Graph()\n",
    "    with graph6.as_default():    \n",
    "        tf_test_dataset=tf.placeholder(tf.float32,shape=(None,None))\n",
    "        #test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved),nn2_weights3_saved)+nn2_biases3_saved)    \n",
    "        \n",
    "    with tf.Session(graph=graph6) as session6:\n",
    "        tf.initialize_all_variables().run()\n",
    "        feed_dict={tf_test_dataset:nn1_single_test_predictions_grams}\n",
    "        nn2_single_test_predictions=session6.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    \n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,1]==1])\n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,2]==1])\n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,3]==1])\n",
    "    print(\" \")\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,1]==1])\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,2]==1])\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,3]==1])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4717\n",
      "[u'Ardiel' u'CABRERA' u'RUIZ' u'Kaidi' u'TOOMUS' u'Julio' u'LAPORTA']\n",
      "[u'Ardiel' u'CABRERA' u'RUIZ' u'Kaidi' u'TOOMUS' u'Julio' u'LAPORTA']\n"
     ]
    }
   ],
   "source": [
    "print len(single_test_tokens)\n",
    "print single_test_tokens[nn1_single_test_predictions[:,0]!=1]\n",
    "print single_test_tokens[nn2_single_test_predictions[:,0]!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_1_db=full_data.copy(deep=True)\n",
    "predictions_2_db=full_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "80\n",
      "Loreto URRACA\n",
      "LUQUE Ana PAREJA\n",
      "Paul BULLOCK\n",
      " \n",
      "Loreto URRACA\n",
      "LUQUE Ana PAREJA\n",
      "Paul BULLOCK\n",
      " \n",
      " \n",
      "81\n",
      "Marcela Langerová\n",
      "Ricardo Nunes Ferreira\n",
      "Janja Felc\n",
      " \n",
      "Marcela Langerová\n",
      "Ricardo Nunes\n",
      "Ferreira Janja Felc\n",
      " \n",
      " \n",
      "82\n",
      "David LEFFLER\n",
      "Andrea NEWMAN\n",
      "Raoul ERARD\n",
      " \n",
      "David LEFFLER\n",
      "Andrea\n",
      "Raoul ERARD\n",
      " \n",
      " \n",
      "83\n",
      "David LEFFLER\n",
      "Hugh O’NEILL\n",
      "Lucie TRPÍKOVÁ\n",
      " \n",
      "David LEFFLER\n",
      "Hugh O’NEILL\n",
      "Lucie TRPÍKOVÁ\n",
      " \n",
      " \n",
      "84\n",
      "Ardiel CABRERA\n",
      "RUIZ Kaidi TOOMUS\n",
      "Julio LAPORTA\n",
      " \n",
      "Ardiel CABRERA\n",
      "RUIZ Kaidi\n",
      "TOOMUS Julio LAPORTA\n",
      " \n",
      " \n",
      "85\n",
      "Deirdre QUINN\n",
      "Sandrine\n",
      "Paul BULLOCK\n",
      " \n",
      "Deirdre QUINN\n",
      "Sandrine BUORO\n",
      "Paul BULLOCK\n",
      " \n",
      " \n",
      "86\n",
      "dissect Swetlana BRAUN\n",
      "Ewelina\n",
      "\n",
      " \n",
      "Swetlana BRAUN\n",
      "Ewelina ŚLIWIŃSKA\n",
      "Silvie\n",
      " \n",
      " \n",
      "87\n",
      "Daniel GÁJA\n",
      "Mira\n",
      "RAJH Radka STUPKOVÁ\n",
      " \n",
      "Daniel GÁJA\n",
      "Mira\n",
      "Radka STUPKOVÁ\n",
      " \n",
      " \n",
      "88\n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Margarida CHAMBEL\n",
      " \n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Margarida CHAMBEL\n",
      " \n",
      " \n",
      "89\n",
      "lsabel DE\n",
      "ALFONSETI URRACA LUQUE Alvaro\n",
      "Loreto SESMA MERINO\n",
      " \n",
      "lsabel DE ALFONSETI\n",
      "URRACA LUQUE\n",
      "Alvaro SESMA MERINO\n",
      " \n",
      " \n",
      "90\n",
      "Patricia LOPEZ\n",
      "FERNANDEZ CORRES Holger\n",
      "Vanessa PAGE\n",
      " \n",
      "Patricia LOPEZ\n",
      "\n",
      "Holger KUNZ Vanessa PAGE\n",
      " \n",
      " \n",
      "91\n",
      "Manuela MIEHLE\n",
      "Richard THEWLIS\n",
      "Daniel GÁJA\n",
      " \n",
      "Manuela MIEHLE\n",
      "Richard THEWLIS\n",
      "Daniel GÁJA\n",
      " \n",
      " \n",
      "92\n",
      "Jorge NOVAIS\n",
      "GONÇALVES Dorothee\n",
      "SCHLIEPHAKE Elena FRAILE\n",
      " \n",
      "Jorge NOVAIS\n",
      "GONÇALVES Dorothee\n",
      "SCHLIEPHAKE Elena FRAILE\n",
      " \n",
      " \n",
      "93\n",
      "Dorothée SCHLIEPHAKE\n",
      "Luca RAMPINI\n",
      "Jorge NOVAIS GONÇALVES\n",
      " \n",
      "Dorothée SCHLIEPHAKE\n",
      "Luca RAMPINI\n",
      "Jorge NOVAIS GONÇALVES\n",
      " \n",
      " \n",
      "94\n",
      "Daniel CAMPAGNE\n",
      "Kelly-Marie BENNETT\n",
      "Daniel GAJA\n",
      " \n",
      "Daniel CAMPAGNE\n",
      "Kelly-Marie BENNETT\n",
      "Daniel GAJA\n",
      " \n",
      " \n",
      "95\n",
      "Liliya Yordanova\n",
      "Alexander Wickenhöfer Thomas\n",
      "D. Nicholson\n",
      " \n",
      "Liliya Yordanova\n",
      "Alexander Wickenhöfer\n",
      "D. Nicholson\n",
      " \n",
      " \n",
      "96\n",
      "Alessandro SEMPIO\n",
      "Mauro BUFFalO\n",
      "luca RAMPINI\n",
      " \n",
      "Alessandro SEMPIO\n",
      "Mauro\n",
      "BUFFalO luca RAMPINI\n",
      " \n",
      " \n",
      "97\n",
      "Susanna LAITINEN\n",
      "Mira\n",
      "Silvié GÖTZOVA\n",
      " \n",
      "Susanna LAITINEN\n",
      "Mira\n",
      "Silvié GÖTZOVA\n",
      " \n",
      " \n",
      "98\n",
      "Miroslav STANČÍK\n",
      "Ric WASLEY\n",
      "Silvie GOTZOVA\n",
      " \n",
      "Miroslav STANČÍK\n",
      "Ric WASLEY\n",
      "Silvie GOTZOVA\n",
      " \n",
      " \n",
      "99\n",
      "Andrea NEWMAN\n",
      "Ewa MISZCZAK\n",
      "Raoul ERARD\n",
      " \n",
      "Andrea NEWMAN\n",
      "Ewa\n",
      "MISZCZAK Raoul ERARD\n",
      " \n",
      " \n",
      "100\n",
      "Alvaro SESMA\n",
      "MERINO Birgit FILTENBORG\n",
      ".\n",
      " \n",
      "Alvaro SESMA\n",
      "MERINO Birgit FILTENBORG Loreto URRACA\n",
      "\n",
      " \n",
      " \n",
      "101\n",
      "Julio Laporta\n",
      "Insa Isabel de Alfonseti\n",
      "Ana Pareja\n",
      " \n",
      "Julio Laporta\n",
      "Insa Isabel Alfonseti\n",
      "Ana Pareja\n",
      " \n",
      " \n",
      "102\n",
      "Agueda MAS\n",
      "PASTOR Pedro\n",
      "ARBIZU\n",
      " \n",
      "Agueda MAS\n",
      "PASTOR Pedro JURADO\n",
      "Blanca ARBIZU\n",
      " \n",
      " \n",
      "103\n",
      "José Antonio\n",
      "GARRIDO OTAOLA\n",
      "LINDEGREN Normunds LAMSTERS\n",
      " \n",
      "José Antonio\n",
      "GARRIDO OTAOLA Tomas\n",
      "Normunds LAMSTERS\n",
      " \n",
      " \n",
      "104\n",
      "Justyna GBYL\n",
      "Katarzyna DOMANSKA\n",
      "Margarida CHAMBEL GONÇALVES\n",
      " \n",
      "Justyna GBYL\n",
      "Katarzyna DOMANSKA\n",
      "Margarida CHAMBEL GONÇALVES\n",
      " \n",
      " \n",
      "105\n",
      "Mira RAJH\n",
      "Natascha SEMJEVSKI Ricardo\n",
      "NUNES FERREIRA\n",
      " \n",
      "Mira RAJH\n",
      "Natascha SEMJEVSKI\n",
      "Ricardo NUNES FERREIRA\n",
      " \n",
      " \n",
      "106\n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Sophie PETREQUIN\n",
      " \n",
      "Paul BULLOCK\n",
      "Inés GARCÍA\n",
      "LLEDÓ Sophie PETREQUIN\n",
      " \n",
      " \n",
      "107\n",
      "Peter Quay\n",
      "Daniel Gája\n",
      "Marcela Langerová\n",
      " \n",
      "Peter Quay\n",
      "Daniel Gája\n",
      "Marcela Langerová\n",
      " \n",
      " \n",
      "108\n",
      "Loreto URRACA\n",
      "LUQUE Paul BULLOCK\n",
      "Antoneta CVETIC\n",
      " \n",
      "Loreto URRACA\n",
      "LUQUE Paul BULLOCK\n",
      "Antoneta CVETIC\n",
      " \n",
      " \n",
      "109\n",
      "Daniel GÁJA\n",
      "Janja FELC IBARRA\n",
      "DE DIEGO\n",
      " \n",
      "Daniel GÁJA\n",
      "Janja FELC IBARRA\n",
      "María Belén DE DIEGO\n",
      " \n",
      " \n",
      "110\n",
      "Mira RAJH\n",
      "Vít\n",
      "MAHELKA Radka STUPKOVÁ\n",
      " \n",
      "Mira RAJH\n",
      "Vít\n",
      "MAHELKA Radka STUPKOVÁ\n",
      " \n",
      " \n",
      "111\n",
      "Martie GALLE\n",
      "Sigrid DICKMANNS\n",
      "Alessandro SEMPIO\n",
      " \n",
      "Martie GALLE\n",
      "Sigrid DICKMANNS\n",
      "Alessandro SEMPIO\n",
      " \n",
      " \n",
      "112\n",
      "\n",
      "\n",
      "GOTZOVA\n",
      " \n",
      "\n",
      "\n",
      "GOTZOVA\n",
      " \n",
      " \n",
      "113\n",
      "Thomás\n",
      "Maria Luisa\n",
      "\n",
      " \n",
      "Thomás LAS\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "114\n",
      "Szabolcs KISS\n",
      "Michaela Pedro\n",
      "SIMANDLOVA DUARTE GUIMARAES\n",
      " \n",
      "Szabolcs KISS\n",
      "Michaela SIMANDLOVA\n",
      "Pedro DUARTE GUIMARAES\n",
      " \n",
      " \n",
      "115\n",
      "Jorge NOVAIS\n",
      "GONÇALVES Carmen CAVADA IPIÑA\n",
      "Nathan ABRAHAM\n",
      " \n",
      "Jorge NOVAIS\n",
      "GONÇALVES Carmen CAVADA\n",
      "IPIÑA Nathan ABRAHAM\n",
      " \n",
      " \n",
      "116\n",
      "Elena FRAILE\n",
      "MUÑOZ Valentín ALONSO MORENO\n",
      "Reiner SARAPOGLU\n",
      " \n",
      "Elena FRAILE\n",
      "MUÑOZ Valentín ALONSO\n",
      "MORENO Reiner SARAPOGLU\n",
      " \n",
      " \n",
      "117\n",
      "Daniel Gája\n",
      "Yvonne Fuxius\n",
      "Radka Stupková\n",
      " \n",
      "Daniel Gája\n",
      "Yvonne\n",
      "Fuxius Radka Stupková\n",
      " \n",
      " \n",
      "118\n",
      "Alistair BUGEJA\n",
      "Daniel GÁJA\n",
      "Sarah WEBER\n",
      " \n",
      "Alistair BUGEJA\n",
      "Daniel GÁJA\n",
      "Sarah WEBER\n",
      " \n",
      " \n",
      "119\n",
      "Alistair Bugeja\n",
      "Lucie Trpíková\n",
      "Margarida Chambel Gonçalves\n",
      " \n",
      "Alistair Bugeja\n",
      "Lucie Trpíková\n",
      "Margarida Chambel Gonçalves\n",
      " \n",
      " \n",
      "120\n",
      "Panayotis Geroulakos\n",
      "Jaime Cos Codina Birgit Holst\n",
      "Filtenborg\n",
      " \n",
      "Panayotis Geroulakos\n",
      "Jaime Cos Codina Birgit Holst Filtenborg\n",
      "\n",
      " \n",
      " \n",
      "121\n",
      "Raoul ERARD\n",
      "Vanessa PAGE\n",
      "Anne-Lee KRISTENSEN\n",
      " \n",
      "Raoul ERARD\n",
      "Vanessa\n",
      "PAGE Anne-Lee KRISTENSEN\n",
      " \n",
      " \n",
      "122\n",
      "Ralph Pethke\n",
      "Reiner\n",
      "Sarapoglu Arne Führer\n",
      " \n",
      "Ralph Pethke\n",
      "Reiner\n",
      "Arne Führer\n",
      " \n",
      " \n",
      "123\n",
      "Normunds LAMSTERS\n",
      "Lauma BUKA\n",
      "André POHLMANN\n",
      " \n",
      "Normunds LAMSTERS\n",
      "Lauma\n",
      "André POHLMANN\n",
      " \n",
      " \n",
      "124\n",
      "Normunds LAMSTERS\n",
      "Angel ESCRIBANO SALVADOR\n",
      "André POHLMANN\n",
      " \n",
      "Normunds LAMSTERS\n",
      "Angel ESCRIBANO\n",
      "SALVADOR André POHLMANN\n",
      " \n",
      " \n",
      "125\n",
      "Anna Gobetto\n",
      "Luca\n",
      "Rampini\n",
      " \n",
      "Anna Gobetto\n",
      "Luca Rampini\n",
      "\n",
      " \n",
      " \n",
      "126\n",
      "Alain RASSAT\n",
      "Paul BULLOCK Luce\n",
      "Maria CAPOSTAGNO\n",
      " \n",
      "Alain RASSAT\n",
      "Paul BULLOCK Maria Luce\n",
      "CAPOSTAGNO\n",
      " \n",
      " \n",
      "127\n",
      "Andrea Newman\n",
      "Catherine Medina\n",
      "Karin Kuhl\n",
      " \n",
      "Andrea Newman\n",
      "Catherine Medina\n",
      "Karin Kuhl\n",
      " \n",
      " \n",
      "128\n",
      "Daniel GAJA SARAPOGLU\n",
      "Reiner\n",
      "DICKMANNS\n",
      " \n",
      "Daniel GAJA\n",
      "Reiner\n",
      "SARAPOGLU DICKMANNS\n",
      " \n",
      " \n",
      "129\n",
      "Vít Mahelka\n",
      "Daniel Gája\n",
      "Marcela Langerová\n",
      " \n",
      "Vít Mahelka\n",
      "Daniel\n",
      "Marcela Langerová\n",
      " \n",
      " \n",
      "130\n",
      "Frédérique SULPICE\n",
      "Zuzanna STOJKOWICZ\n",
      "Alexandre BERSET\n",
      " \n",
      "Frédérique SULPICE\n",
      "Zuzanna\n",
      "STOJKOWICZ Alexandre BERSET\n",
      " \n",
      " \n",
      "131\n",
      "Tamas Kocsis\n",
      "Jorge Novais Gonçalves\n",
      "Reiner Sarapoglu\n",
      " \n",
      "Tamas Kocsis\n",
      "Jorge Novais\n",
      "Gonçalves Reiner Sarapoglu\n",
      " \n",
      " \n",
      "132\n",
      "Pedro JURADO MONTEJANO\n",
      "Catherine\n",
      "MEDINA Frédérique SULPICE\n",
      " \n",
      "Pedro JURADO\n",
      "MONTEJANO Catherine\n",
      "Frédérique SULPICE\n",
      " \n",
      " \n",
      "133\n",
      "lsabel DE\n",
      ". ALFONSETI HARTMANN\n",
      "Alvaro SESMA\n",
      " \n",
      "lsabel DE ALFONSETI\n",
      "\n",
      "HARTMANN Alvaro SESMA\n",
      " \n",
      " \n",
      "134\n",
      "Vft MAHELKA\n",
      "Alistair BUGEJA Angel\n",
      "ESCRIBANO SALVADOR\n",
      " \n",
      "Vft MAHELKA\n",
      "Alistair BUGEJA\n",
      "ESCRIBANO SALVADOR\n",
      " \n",
      " \n",
      "135\n",
      "Oscar MONDEJAR\n",
      "ORTUÑO Panayotis\n",
      "GEROULAKOS Wolfgang SCHRAMEK\n",
      " \n",
      "Oscar MONDEJAR\n",
      "ORTUÑO Panayotis\n",
      "GEROULAKOS Wolfgang SCHRAMEK\n",
      " \n",
      " \n",
      "136\n",
      "Oscar MONDEJAR\n",
      "Colette\n",
      "HANCK Margarida CHAMBEL\n",
      " \n",
      "Oscar MONDEJAR\n",
      "Colette\n",
      "HANCK Margarida CHAMBEL\n",
      " \n",
      " \n",
      "137\n",
      "Justas IVANAUSKAS\n",
      "Zuzana WALTHER\n",
      "Silvie GOTZOVA\n",
      " \n",
      "Justas IVANAUSKAS\n",
      "Zuzana\n",
      "Silvie GOTZOVA\n",
      " \n",
      " \n",
      "138\n",
      "Birgit FILTENBORG\n",
      "Ioana\n",
      "Sarah WEBER\n",
      " \n",
      "Birgit FILTENBORG\n",
      "Ioana\n",
      "MOISESCU Sarah WEBER\n",
      " \n",
      " \n",
      "139\n",
      "José Antonio\n",
      "GARRIDO OTAOLA\n",
      "LINDEGREN Normunds LAMSTERS\n",
      " \n",
      "José Antonio\n",
      "GARRIDO OTAOLA Tomas\n",
      "Normunds LAMSTERS\n",
      " \n",
      " \n",
      "140\n",
      "Markus Kicia\n",
      "Schlie\n",
      "Lee Scott\n",
      " \n",
      "Markus Kicia\n",
      "Claudia Schlie\n",
      "Lee Scott\n",
      " \n",
      " \n",
      "141\n",
      "Michaela Simandlova\n",
      "Szabolcs Kiss\n",
      "Asta Lukosiute\n",
      " \n",
      "Michaela Simandlova\n",
      "Szabolcs Kiss\n",
      "Asta Lukosiute\n",
      " \n",
      " \n",
      "142\n",
      "Vanessa PAGE\n",
      "\n",
      "LINDEGREN Ewa MISZCZAK\n",
      " \n",
      "\n",
      "Tomas\n",
      "LINDEGREN Ewa MISZCZAK\n",
      " \n",
      " \n",
      "143\n",
      "Markus KICIA\n",
      "Martin LAYTON\n",
      "Claudia SCHLIE\n",
      " \n",
      "Markus KICIA\n",
      "Martin LAYTON\n",
      "Claudia SCHLIE\n",
      " \n",
      " \n",
      "144\n",
      "Sophie Petrequin\n",
      "Susana Palmero Cabezas\n",
      "Karin Kuhl\n",
      " \n",
      "only Sophie Petrequin\n",
      "Susana Palmero Cabezas\n",
      "Karin Kuhl\n",
      " \n",
      " \n",
      "145\n",
      "Janja FELC\n",
      "Mira RAJH\n",
      "Natascha SEMJEVSKI\n",
      " \n",
      "Janja FELC\n",
      "Mira RAJH\n",
      "Natascha SEMJEVSKI\n",
      " \n",
      " \n",
      "146\n",
      "Mariya KOLEVA\n",
      ".\n",
      "Asta LUKOSIOTE\n",
      " \n",
      "Mariya KOLEVA\n",
      "\n",
      "Asta LUKOSIOTE\n",
      " \n",
      " \n",
      "147\n",
      "Daniel GAJA\n",
      "Mira\n",
      "RAJH Steven STAM\n",
      " \n",
      "Daniel GAJA\n",
      "Mira\n",
      "RAJH Steven STAM\n",
      " \n",
      " \n",
      "148\n",
      "Cinzia NEGRO\n",
      "Nicole CLARKE\n",
      "Julio LAPORTA INSA\n",
      " \n",
      "Cinzia NEGRO\n",
      "Nicole CLARKE\n",
      "Julio LAPORTA INSA\n",
      " \n",
      " \n",
      "149\n",
      "Agueda MAS\n",
      "PASTOR Pedro JURADO MONTEJANO Blanca\n",
      "ARTECHE ARBIZU\n",
      " \n",
      "Agueda MAS\n",
      "PASTOR Pedro JURADO MONTEJANO\n",
      "Blanca ARTECHE ARBIZU\n",
      " \n",
      " \n",
      "150\n",
      "Oscar MONDEJAR\n",
      "ORTUÑO Panayotis\n",
      "GEROULAKOS Ruxandra MANEA :\n",
      " \n",
      "Oscar MONDEJAR\n",
      "ORTUÑO Panayotis\n",
      "GEROULAKOS Ruxandra MANEA\n",
      " \n",
      " \n",
      "151\n",
      "Ieva Smilgaine\n",
      "Thomas D. Nicholson\n",
      "Alexander Wickenhöfer\n",
      " \n",
      "Ieva Smilgaine\n",
      "Thomas D. Nicholson\n",
      "Alexander Wickenhöfer\n",
      " \n",
      " \n",
      "152\n",
      "Bo ZACHRISSON\n",
      "Angel ESCRIBANO\n",
      "Claudia SCHLIE\n",
      " \n",
      "Bo ZACHRISSON\n",
      "Angel ESCRIBANO\n",
      "Claudia SCHLIE\n",
      " \n",
      " \n",
      "153\n",
      "Tuomas Mattila\n",
      "Escribano Salvador\n",
      "Peter Quay\n",
      " \n",
      "Tuomas Mattila\n",
      "Angel Escribano\n",
      "Peter Quay\n",
      " \n",
      " \n",
      "154\n",
      "Paul BULLOCK\n",
      "Sophie PETREQUIN\n",
      "\n",
      " \n",
      "Paul BULLOCK\n",
      "Sophie PETREQUIN\n",
      "Margarida\n",
      " \n",
      " \n",
      "155\n",
      "Jörg WEBERNDÖRFER\n",
      "Claudia SCHLIE\n",
      "Chantal VAN RIEL\n",
      " \n",
      "Jörg WEBERNDÖRFER\n",
      "Claudia SCHLIE Chantal\n",
      "VAN RIEL\n",
      " \n",
      " \n",
      "156\n",
      "Daniel GÁJA\n",
      "Radka STUPKOVÁ\n",
      "Richard THEWLIS\n",
      " \n",
      "Daniel GÁJA\n",
      "Radka STUPKOVÁ\n",
      "Richard THEWLIS\n",
      " \n",
      " \n",
      "157\n",
      "Neil RICHARDS\n",
      "David LEFFLER\n",
      "Raoul ERARD\n",
      " \n",
      "Neil RICHARDS\n",
      "David LEFFLER\n",
      "Raoul ERARD\n",
      " \n",
      " \n",
      "158\n",
      "Tuomas Mattila\n",
      "Lucinda Carney\n",
      "Justas lvanauskas\n",
      " \n",
      "Tuomas Mattila\n",
      "Lucinda\n",
      "Justas lvanauskas\n",
      " \n",
      " \n",
      "159\n",
      "Ana PAREJA\n",
      "Loreto URRACA LUQUE Alvaro\n",
      "SESMA MERINO\n",
      " \n",
      "Ana PAREJA\n",
      "Loreto URRACA LUQUE\n",
      "Alvaro SESMA MERINO\n",
      " \n",
      " \n",
      "160\n",
      "Mira RAJH\n",
      "Vít\n",
      "MAHELKA Radka STUPKOVÁ\n",
      " \n",
      "Mira RAJH\n",
      "Vít\n",
      "MAHELKA Radka STUPKOVÁ\n",
      " \n",
      " \n",
      "161\n",
      "Daniel GÁJA\n",
      "Eva PERINOVA\n",
      "Tuomas MATTILA\n",
      " \n",
      "Daniel GÁJA\n",
      "Eva PERINOVA\n",
      "Tuomas MATTILA\n",
      " \n",
      " \n",
      "162\n",
      "Ardiel CABRERA\n",
      "RUIZ Marianna KONDÁS\n",
      "Julio LAPORTA\n",
      " \n",
      "Ardiel CABRERA\n",
      "RUIZ Marianna\n",
      "KONDÁS Julio LAPORTA\n",
      " \n",
      " \n",
      "163\n",
      "Daniel Gaja\n",
      "Liliya Thewlis\n",
      "Yordanova\n",
      " \n",
      "Daniel Gaja\n",
      "Liliya Yordanova\n",
      "Richard\n",
      " \n",
      " \n",
      "164\n",
      "Peter QUAY\n",
      "Martin LENZ\n",
      "Normunds LAMSTERS\n",
      " \n",
      "Peter QUAY\n",
      "Martin LENZ\n",
      "Normunds LAMSTERS\n",
      " \n",
      " \n",
      "165\n",
      "PAGE\n",
      "Miroslav\n",
      "STANČÍK Martin FISCHER\n",
      " \n",
      "\n",
      "Miroslav\n",
      "59262 STANČÍK Martin FISCHER\n",
      " \n",
      " \n",
      "166\n",
      "Frédérique SULPICE\n",
      "André\n",
      "POHLMANN Marc BURKI\n",
      " \n",
      "Frédérique SULPICE\n",
      "André\n",
      "POHLMANN Marc BURKI\n",
      " \n",
      " \n",
      "167\n",
      "Dorothee Schliephake\n",
      "Reiner Sarapoglu\n",
      "Arne FOhrer\n",
      " \n",
      "Dorothee Schliephake\n",
      "Reiner Sarapoglu\n",
      "Arne FOhrer\n",
      " \n",
      " \n",
      "168\n",
      "Tuomas MATTILA\n",
      "Mira RAJH\n",
      "Erkki MÜNTER\n",
      " \n",
      "Tuomas MATTILA\n",
      "Mira RAJH\n",
      "Erkki MÜNTER\n",
      " \n",
      " \n",
      "169\n",
      "Vít MAHELKA\n",
      "Mette WINTERSKOV\n",
      "Peter SCHYDLOWSKI\n",
      " \n",
      "Vít MAHELKA\n",
      "Mette\n",
      "Peter SCHYDLOWSKI\n",
      " \n",
      " \n",
      "170\n",
      "Ignacio IGLESIAS\n",
      "ARROYO Agueda MAS PASTOR José Antonio\n",
      "GARRIDO OTAOLA\n",
      " \n",
      "Ignacio IGLESIAS\n",
      "ARROYO Agueda MAS PASTOR José\n",
      "GARRIDO OTAOLA\n",
      " \n",
      " \n",
      "171\n",
      "Daniel GÁJA\n",
      "Anna POLITI\n",
      "Beatrix STELTER\n",
      " \n",
      "Daniel GÁJA\n",
      "Anna POLITI\n",
      "Beatrix STELTER\n",
      " \n",
      " \n",
      "172\n",
      "Katarzyna DOMANSKA\n",
      "Mihály DARIDA\n",
      "Marianna KONDÁS\n",
      " \n",
      "Katarzyna DOMANSKA\n",
      "Mihály\n",
      "DARIDA Marianna KONDÁS\n",
      " \n",
      " \n",
      "173\n",
      "Alessandro SEMPIO\n",
      "ALONSO MORENO\n",
      "Ric WASLEY\n",
      " \n",
      "Alessandro SEMPIO\n",
      "Valentín ALONSO\n",
      "MORENO Ric WASLEY\n",
      " \n",
      " \n",
      "174\n",
      "Zuzana WAL\n",
      "THER Szabolcs KISS\n",
      "Alvaro SESMA\n",
      " \n",
      "Zuzana WAL\n",
      "THER Szabolcs KISS\n",
      "Alvaro SESMA\n",
      " \n",
      " \n",
      "175\n",
      "Daniel GAJA\n",
      "Mira\n",
      "RAJH Steven STAM\n",
      " \n",
      "Daniel GAJA\n",
      "Mira\n",
      "RAJH Steven STAM\n",
      " \n",
      " \n",
      "176\n",
      "Richard THEWLIS\n",
      "William\n",
      "TROTT Helena NOKKANEN\n",
      " \n",
      "Richard THEWLIS\n",
      "William\n",
      "TROTT Helena NOKKANEN\n",
      " \n",
      " \n",
      "177\n",
      "Juan MORALES\n",
      "Anna GOBETTO\n",
      "Mauro BUFFOLO\n",
      " \n",
      "Juan MORALES\n",
      "Anna GOBETTO\n",
      "Mauro BUFFOLO\n",
      " \n",
      " \n",
      "178\n",
      "Frederique SULPICE\n",
      "\n",
      "[ KOVA\n",
      " \n",
      "Frederique SULPICE\n",
      "Vanessa\n",
      "TRP KOVA\n",
      " \n",
      " \n",
      "179\n",
      "Anna GOBETTO\n",
      "Alessandro SEMPIO\n",
      "Luca RAMPINI\n",
      " \n",
      "Anna GOBETTO\n",
      "Alessandro\n",
      "Luca RAMPINI\n",
      " \n",
      " \n",
      "180\n",
      "Wolfgang SCHRAMEK\n",
      "Zoltán\n",
      "SZARKA Deirdre QUINN\n",
      " \n",
      "Wolfgang SCHRAMEK\n",
      "Zoltán\n",
      "SZARKA Deirdre QUINN\n",
      " \n",
      " \n",
      "181\n",
      "Daniel GÁJA\n",
      "Richard\n",
      "THEWLIS Manuela MIEHLE\n",
      " \n",
      "Daniel GÁJA\n",
      "Richard\n",
      "THEWLIS Manuela MIEHLE\n",
      " \n",
      " \n",
      "182\n",
      "Jorge NOVAIS\n",
      "GONÇALVES Péter SIPOS Ana\n",
      "María PAREJA TORRES\n",
      " \n",
      "Jorge NOVAIS\n",
      "GONÇALVES Péter SIPOS Ana\n",
      "María PAREJA TORRES\n",
      " \n",
      " \n",
      "183\n",
      "Daniel Gája\n",
      "Tuomas Mattila Ricardo\n",
      "Nunes Ferreira\n",
      " \n",
      "Daniel Gája\n",
      "Tuomas Mattila\n",
      "Ricardo Nunes Ferreira\n",
      " \n",
      " \n",
      "184\n",
      "Marianna KONDÁS\n",
      "Mihály DARIDA Ardiel\n",
      "CABRERA RUIZ\n",
      " \n",
      "Marianna KONDÁS\n",
      "Mihály DARIDA\n",
      "Ardiel CABRERA RUIZ\n",
      " \n",
      " \n",
      "185\n",
      "Ioana MOISESCU\n",
      "Paul\n",
      "BULLOCK Laurent BEAUSSE\n",
      " \n",
      "Ioana MOISESCU\n",
      "Paul\n",
      "BULLOCK Laurent BEAUSSE\n",
      " \n",
      " \n",
      "186\n",
      "Kelly BENNETT\n",
      "Marcela LANGEROVÁ\n",
      "Richard THEWLIS\n",
      " \n",
      "Kelly BENNETT\n",
      "Marcela LANGEROVÁ\n",
      "Richard THEWLIS\n",
      " \n",
      " \n",
      "187\n",
      "Wolfgang SCHRAMEK\n",
      "Martin FISCHER\n",
      "S.a. Raoul ERARD\n",
      " \n",
      "Wolfgang SCHRAMEK\n",
      "Martin\n",
      "FISCHER Raoul ERARD\n",
      " \n",
      " \n",
      "188\n",
      "Vanessa PAGE\n",
      "OTAOLA Pedro\n",
      "JURADO MONTEJANO\n",
      " \n",
      "\n",
      "José OTAOLA\n",
      "Antonio Pedro JURADO MONTEJANO\n",
      " \n",
      " \n",
      "189\n",
      "Silvie GOTZOVA\n",
      "Tomas ZIAK\n",
      "Swetlana BRAUN\n",
      " \n",
      "Silvie GOTZOVA\n",
      "Tomas\n",
      "ZIAK Swetlana BRAUN\n",
      " \n",
      " \n",
      "190\n",
      "Alistair Bugeja\n",
      "Lucie Trpíková\n",
      "Margarida Chambel Gonçalves\n",
      " \n",
      "Alistair Bugeja\n",
      "Lucie Trpíková\n",
      "Margarida Chambel Gonçalves\n",
      " \n",
      " \n",
      "191\n",
      "Marcela LANGEROVA\n",
      "Ricardo\n",
      "NUNES FERREIRA\n",
      " \n",
      "Marcela LANGEROVA\n",
      "Ricardo\n",
      "NUNES FERREIRA\n",
      " \n",
      " \n",
      "192\n",
      "Frédérique SULPICE\n",
      "Vanessa PAGE\n",
      "Anne-Lee KRISTENSEN\n",
      " \n",
      "Frédérique SULPICE\n",
      "Vanessa\n",
      "PAGE Anne-Lee KRISTENSEN\n",
      " \n",
      " \n",
      "193\n",
      "Péter SIPOS\n",
      "Birgit Holst Ana\n",
      "María PAREJA TORRES\n",
      " \n",
      "Péter SIPOS\n",
      "Birgit Holst FILTENBORG Ana\n",
      "María PAREJA TORRES\n",
      " \n",
      " \n",
      "194\n",
      "Alexandre BERSET\n",
      "Tamás\n",
      "Normunds LAMSTERS\n",
      " \n",
      "Alexandre BERSET\n",
      "Tamás\n",
      "SOMLAI Normunds LAMSTERS\n",
      " \n",
      " \n",
      "195\n",
      "Frédérique SULPICE\n",
      "Raoul ERARD\n",
      "Andrea NEWMAN\n",
      " \n",
      "Frédérique SULPICE\n",
      "Raoul\n",
      "ERARD Andrea NEWMAN\n",
      " \n",
      " \n",
      "196\n",
      "Alessandro SEMPIO\n",
      "Anna GOBETTO\n",
      "Luca RAMPINI\n",
      " \n",
      "Alessandro SEMPIO\n",
      "Anna GOBETTO\n",
      "Luca RAMPINI\n",
      " \n",
      " \n",
      "197\n",
      "Catherine MEDINA\n",
      "Zuzanna STOJKOWICZ\n",
      "concerning Raoul ERARD\n",
      " \n",
      "Catherine MEDINA\n",
      "Zuzanna\n",
      "STOJKOWICZ Raoul ERARD\n",
      " \n",
      " \n",
      "198\n",
      "Carmen CAVADA\n",
      "IPIÑA Birgit HOLST FILTENBORG\n",
      "Julio LAPORTA INSA\n",
      " \n",
      "Carmen CAVADA\n",
      "IPIÑA Birgit HOLST FILTENBORG\n",
      "Julio LAPORTA INSA\n",
      " \n",
      " \n",
      "199\n",
      "Lucinda CARNEY\n",
      "Daniel GÁJA\n",
      "Eva PERINOVA\n",
      " \n",
      "Lucinda CARNEY\n",
      "Daniel GÁJA\n",
      "Eva PERINOVA\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in range(80,200):\n",
    "    print(\" \")\n",
    "    print index\n",
    "    single_test_dataset=data_features[index].astype(\"float32\")\n",
    "    single_test_labels=data_labels[index]\n",
    "    single_test_tokens=np.asarray(data_tokens[index])\n",
    "    \n",
    "    graph5=tf.Graph()\n",
    "    with graph5.as_default():    \n",
    "        tf_test_dataset=tf.placeholder(tf.float32,shape=(None,None))\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn1_weights1) + nn1_biases1),nn1_weights2)+nn1_biases2),nn1_weights3)+nn1_biases3)    \n",
    "        \n",
    "    with tf.Session(graph=graph5) as session5:\n",
    "        tf.initialize_all_variables().run()  \n",
    "        feed_dict={tf_test_dataset:single_test_dataset}\n",
    "        nn1_single_test_predictions=session5.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "        \n",
    "    nn1_single_test_predictions_grams=nn2_input_feature_extraction(nn1_single_test_predictions,context_len_nn_2,num_labels)\n",
    "    nn1_single_test_predictions_grams=np.hstack((nn1_single_test_predictions_grams,single_test_dataset))\n",
    "    \n",
    "    graph6=tf.Graph()\n",
    "    with graph6.as_default():    \n",
    "        tf_test_dataset=tf.placeholder(tf.float32,shape=(None,None))\n",
    "        #test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved)\n",
    "        test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.sigmoid(tf.matmul(tf_test_dataset, nn2_weights1_saved) + nn2_biases1_saved),nn2_weights2_saved)+nn2_biases2_saved),nn2_weights3_saved)+nn2_biases3_saved)    \n",
    "        \n",
    "    with tf.Session(graph=graph6) as session6:\n",
    "        tf.initialize_all_variables().run()\n",
    "        feed_dict={tf_test_dataset:nn1_single_test_predictions_grams}\n",
    "        nn2_single_test_predictions=session6.run([test_prediction],feed_dict=feed_dict)[0]\n",
    "    \n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,1]==1])\n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,2]==1])\n",
    "    print ' '.join(single_test_tokens[nn1_single_test_predictions[:,3]==1])\n",
    "    print(\" \")\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,1]==1])\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,2]==1])\n",
    "    print ' '.join(single_test_tokens[nn2_single_test_predictions[:,3]==1])\n",
    "\n",
    "    predictions_1_db.loc[index,\"judge_name_1\"]= ' '.join(single_test_tokens[nn1_single_test_predictions[:,1]==1])\n",
    "    predictions_1_db.loc[index,\"judge_name_2\"]= ' '.join(single_test_tokens[nn1_single_test_predictions[:,2]==1])\n",
    "    predictions_1_db.loc[index,\"judge_name_3\"]= ' '.join(single_test_tokens[nn1_single_test_predictions[:,3]==1])\n",
    "    print(\" \")\n",
    "    predictions_2_db.loc[index,\"judge_name_1\"]= ' '.join(single_test_tokens[nn2_single_test_predictions[:,1]==1])\n",
    "    predictions_2_db.loc[index,\"judge_name_2\"]= ' '.join(single_test_tokens[nn2_single_test_predictions[:,2]==1])\n",
    "    predictions_2_db.loc[index,\"judge_name_3\"]= ' '.join(single_test_tokens[nn2_single_test_predictions[:,3]==1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "writer = ExcelWriter('euipo_english_judges_pred_1.xlsx',encoding='unicode')\n",
    "predictions_1_db.loc[:,\"filename\":\"judge_name_3\"].to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "writer = ExcelWriter('euipo_english_judges_pred_2.xlsx',encoding='unicode')\n",
    "predictions_2_db.loc[:,\"filename\":\"judge_name_3\"].to_excel(writer,'Sheet2')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
